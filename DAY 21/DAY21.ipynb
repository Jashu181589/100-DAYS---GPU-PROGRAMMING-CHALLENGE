{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92c9869",
   "metadata": {},
   "source": [
    "# DAY 21: Stochastic Gradient Descent (SGD) in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sgd_cuda.cu\n",
    "// nvcc sgd_cuda.cu -o sgd_cuda\n",
    "\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "#include <curand_kernel.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "\n",
    "// CUDA kernel to compute predictions and squared loss\n",
    "__global__ void compute_loss(float* X, float* y, float* W, float* b, float* loss, float* y_pred, int N, int D) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {  \n",
    "        float y_pred_val = 0.0f;\n",
    "        for (int i = 0; i < D; i++) {\n",
    "            y_pred_val += X[idx * D + i] * W[i];\n",
    "        }\n",
    "        y_pred_val += *b; // Use single scalar bias\n",
    "        y_pred[idx] = y_pred_val;\n",
    "        loss[idx] = (y[idx] - y_pred_val) * (y[idx] - y_pred_val); // Squared loss\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel to compute gradients\n",
    "__global__ void compute_gradients(float* X, float* loss, float* dW, float* db, int N, int D) {\n",
    "    __shared__ float db_shared[BLOCK_SIZE];\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (idx < D) {\n",
    "        float gradW = 0.0f;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            gradW += X[i * D + idx] * loss[i];\n",
    "        }\n",
    "        dW[idx] = - (2.0f / N) * gradW;\n",
    "    }\n",
    "    float gradb = 0.0f;\n",
    "    if (idx < N) {\n",
    "        gradb = loss[idx];\n",
    "    }\n",
    "    db_shared[threadIdx.x] = gradb;\n",
    "    __syncthreads();\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        float sum_db = 0.0f;\n",
    "        for (int i = 0; i < blockDim.x; i++) {\n",
    "            sum_db += db_shared[i];\n",
    "        }\n",
    "        atomicAdd(db, - (2.0f / N) * sum_db);\n",
    "    }\n",
    "}\n",
    "\n",
    "// CUDA kernel to update weights using SGD\n",
    "__global__ void update_weights(float* W, float* dW, float* b, float* db, float lr, int D) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < D) {\n",
    "        W[idx] -= lr * dW[idx];\n",
    "    }\n",
    "    if (idx == 0) {\n",
    "        *b -= lr * (*db);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host function to train the model\n",
    "void train_sgd(float* h_X, float* h_y, float* h_W, float* h_b, int N, int D, float lr, int epochs) {\n",
    "    float *d_X, *d_y, *d_W, *d_b, *d_gradW, *d_gradb, *d_loss, *d_y_pred;\n",
    "    cudaMalloc(&d_X, N * D * sizeof(float));\n",
    "    cudaMalloc(&d_y, N * sizeof(float));\n",
    "    cudaMalloc(&d_W, D * sizeof(float));\n",
    "    cudaMalloc(&d_b, sizeof(float));\n",
    "    cudaMalloc(&d_gradW, D * sizeof(float));\n",
    "    cudaMalloc(&d_gradb, sizeof(float));\n",
    "    cudaMalloc(&d_loss, N * sizeof(float));\n",
    "    cudaMalloc(&d_y_pred, N * sizeof(float));\n",
    "    \n",
    "    cudaMemcpy(d_X, h_X, N * D * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_W, h_W, D * sizeof(float), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, sizeof(float), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "    int blocks_grad = (D + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
    "\n",
    "    for (int epoch = 0; epoch < epochs; ++epoch) {\n",
    "        compute_loss<<<blocks, BLOCK_SIZE>>>(d_X, d_y, d_W, d_b, d_loss, d_y_pred, N, D);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        compute_gradients<<<blocks_grad, BLOCK_SIZE>>>(d_X, d_loss, d_gradW, d_gradb, N, D);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        update_weights<<<blocks_grad, BLOCK_SIZE>>>(d_W, d_gradW, d_b, d_gradb, lr, D);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(h_W, d_W, D * sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b, d_b, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaFree(d_X);\n",
    "    cudaFree(d_y);\n",
    "    cudaFree(d_W);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_gradW);\n",
    "    cudaFree(d_gradb);\n",
    "    cudaFree(d_loss);\n",
    "    cudaFree(d_y_pred);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1024;\n",
    "    int D = 10;\n",
    "    float lr = 0.01;\n",
    "    int epochs = 1000;\n",
    "\n",
    "    float *h_X = new float[N * D];\n",
    "    float *h_y = new float[N];\n",
    "    float *h_W = new float[D];\n",
    "    float *h_b = new float[1];\n",
    "\n",
    "    srand(42);\n",
    "    for (int i = 0; i < N * D; i++) {\n",
    "        h_X[i] = static_cast<float>(rand()) / RAND_MAX;\n",
    "    }\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_y[i] = static_cast<float>(rand()) / RAND_MAX;\n",
    "    }\n",
    "    for (int i = 0; i < D; i++) {\n",
    "        h_W[i] = static_cast<float>(rand()) / RAND_MAX;\n",
    "    }\n",
    "    *h_b = static_cast<float>(rand()) / RAND_MAX;\n",
    "\n",
    "    train_sgd(h_X, h_y, h_W, h_b, N, D, lr, epochs);\n",
    "\n",
    "    std::cout << \"Trained Weights: \";\n",
    "    for (int i = 0; i < D; i++) std::cout << h_W[i] << \" \";\n",
    "    std::cout << \"\\nTrained Bias: \" << *h_b << std::endl;\n",
    "\n",
    "    delete[] h_X;\n",
    "    delete[] h_y;\n",
    "    delete[] h_W;\n",
    "    delete[] h_b;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and run the CUDA SGD implementation\n",
    "!nvcc sgd_cuda.cu -o sgd_cuda\n",
    "!./sgd_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b61a2",
   "metadata": {},
   "source": [
    "## Output:\n",
    "```\n",
    "Trained Weights: 0.394383 0.783099 0.798441 0.911647 0.197551 0.335223 0.768229 0.277775 0.553970 0.477397 \n",
    "Trained Bias: 0.628871\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
