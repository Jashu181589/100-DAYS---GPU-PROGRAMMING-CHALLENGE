{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QBxLA1u9ehB",
        "outputId": "d1bc651a-078e-451a-9a5e-2e23bb48780f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.cu\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define CUDA_CHECK(call) do { \\\n",
        "    cudaError_t err = call; \\\n",
        "    if (err != cudaSuccess) { \\\n",
        "        fprintf(stderr, \"CUDA error in %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
        "                cudaGetErrorString(err)); \\\n",
        "        exit(EXIT_FAILURE); \\\n",
        "    } \\\n",
        "} while(0)\n",
        "\n",
        "__global__ void ELL_kernel(const float* A, const float* X, float* data_ell,\n",
        "                           int* indices_ell, float* data_coo, int* row_coo,\n",
        "                           int* col_coo, float* output_matrix, const int threshold,\n",
        "                           const int N, const int M, int* global_coo_counter) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (row >= N) return;\n",
        "\n",
        "    int counter = 0;\n",
        "\n",
        "    for (int col = 0; col < M; ++col) {\n",
        "        float val = A[row * M + col];\n",
        "        if (val != 0) {\n",
        "            if (counter < threshold) {\n",
        "                data_ell[counter * N + row] = val;\n",
        "                indices_ell[counter * N + row] = col;\n",
        "                counter++;\n",
        "            } else {\n",
        "                int coo_index = atomicAdd(global_coo_counter, 1);\n",
        "                data_coo[coo_index] = val;\n",
        "                row_coo[coo_index] = row;\n",
        "                col_coo[coo_index] = col;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int i = counter; i < threshold; ++i) {\n",
        "        data_ell[i * N + row] = 0;\n",
        "        indices_ell[i * N + row] = -1;\n",
        "    }\n",
        "\n",
        "    float acc = 0.0f;\n",
        "    for (int p = 0; p < threshold; ++p) {\n",
        "        int index = indices_ell[p * N + row];\n",
        "        if (index != -1) {\n",
        "            acc += data_ell[p * N + row] * X[index];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Add COO contribution\n",
        "    for (int i = 0; i < *global_coo_counter; ++i) {\n",
        "        if (row_coo[i] == row) {\n",
        "            acc += data_coo[i] * X[col_coo[i]];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output_matrix[row] = acc;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000;        // Rows (will be replaced by Python)\n",
        "    const int M = 1000;        // Cols (will be replaced by Python)\n",
        "    const int threshold = 20;  // ELL slots per row (will be replaced by Python)\n",
        "\n",
        "    float* A = new float[N * M];\n",
        "    float* data_ell = new float[N * threshold]();\n",
        "    float* data_coo = new float[N * M]();\n",
        "    int* indices_ell = new int[N * threshold]();\n",
        "    int* row_coo = new int[N * M]();\n",
        "    int* col_coo = new int[N * M]();\n",
        "    float* X = new float[M];\n",
        "    float* output_matrix = new float[N];\n",
        "\n",
        "    int* d_global_coo_counter;\n",
        "    CUDA_CHECK(cudaMalloc(&d_global_coo_counter, sizeof(int)));\n",
        "    CUDA_CHECK(cudaMemset(d_global_coo_counter, 0, sizeof(int)));\n",
        "\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < M; j++) {\n",
        "            A[i * M + j] = (i + j) % 3 == 0 ? i + j : 0;\n",
        "        }\n",
        "    }\n",
        "    for (int i = 0; i < M; i++) X[i] = 1.0f;\n",
        "\n",
        "    float *d_A, *d_X, *d_data_ell, *d_data_coo, *d_output_matrix;\n",
        "    int *d_indices_ell, *d_row_coo, *d_col_coo;\n",
        "\n",
        "    CUDA_CHECK(cudaMalloc(&d_A, N * M * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_X, M * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_data_ell, N * threshold * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_data_coo, N * M * sizeof(float)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_indices_ell, N * threshold * sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_row_coo, N * M * sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_col_coo, N * M * sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output_matrix, N * sizeof(float)));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_A, A, N * M * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_X, X, M * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    int block_size = 256;\n",
        "    int num_blocks = (N + block_size - 1) / block_size;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "    CUDA_CHECK(cudaEventRecord(start));\n",
        "\n",
        "    ELL_kernel<<<num_blocks, block_size>>>(d_A, d_X, d_data_ell, d_indices_ell,\n",
        "                                           d_data_coo, d_row_coo, d_col_coo,\n",
        "                                           d_output_matrix, threshold, N, M, d_global_coo_counter);\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(stop));\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "    std::cout << \"CUDA kernel time: \" << milliseconds / 1000.0 << \" seconds\" << std::endl;\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(data_ell, d_data_ell, N * threshold * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(data_coo, d_data_coo, N * M * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(indices_ell, d_indices_ell, N * threshold * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(row_coo, d_row_coo, N * M * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(col_coo, d_col_coo, N * M * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(output_matrix, d_output_matrix, N * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    int h_global_coo_counter;\n",
        "    CUDA_CHECK(cudaMemcpy(&h_global_coo_counter, d_global_coo_counter, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    for (int i = 0; i < 10; ++i) {\n",
        "        std::cout << \"COO[\" << i << \"]: val = \" << data_coo[i] << \", row = \" << row_coo[i] << \", col = \" << col_coo[i] << std::endl;\n",
        "    }\n",
        "\n",
        "    FILE *output_file = fopen(\"cuda_results.txt\", \"w\");\n",
        "    if (output_file == nullptr) {\n",
        "        std::cerr << \"Failed to open output file!\" << std::endl;\n",
        "        return EXIT_FAILURE;\n",
        "    }\n",
        "    for (int i = 0; i < N; i++) fprintf(output_file, \"%.10f\\n\", output_matrix[i]);\n",
        "    fclose(output_file);\n",
        "    std::cout << \"Wrote \" << N << \" values to cuda_results.txt\" << std::endl;\n",
        "\n",
        "    CUDA_CHECK(cudaFree(d_A));\n",
        "    CUDA_CHECK(cudaFree(d_X));\n",
        "    CUDA_CHECK(cudaFree(d_data_ell));\n",
        "    CUDA_CHECK(cudaFree(d_data_coo));\n",
        "    CUDA_CHECK(cudaFree(d_indices_ell));\n",
        "    CUDA_CHECK(cudaFree(d_row_coo));\n",
        "    CUDA_CHECK(cudaFree(d_col_coo));\n",
        "    CUDA_CHECK(cudaFree(d_output_matrix));\n",
        "\n",
        "    delete[] A;\n",
        "    delete[] data_ell;\n",
        "    delete[] data_coo;\n",
        "    delete[] indices_ell;\n",
        "    delete[] row_coo;\n",
        "    delete[] col_coo;\n",
        "    delete[] X;\n",
        "    delete[] output_matrix;\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_benchmark.py\n",
        "import subprocess, numpy as np, torch, time, matplotlib.pyplot as plt, psutil, os, gc\n",
        "\n",
        "def get_memory_info_gb():\n",
        "    m = psutil.virtual_memory()\n",
        "    return m.used / (1024**3), m.total / (1024**3)\n",
        "\n",
        "def estimate_memory_usage_gb(N, M):\n",
        "    # ~1/3 nonzeros for (i+j)%3==0, PyTorch uses 64-bit indices, float32 values\n",
        "    nnz = (N * M) // 3\n",
        "    bytes_per_nnz = 8 + 8 + 4  # row idx + col idx + value\n",
        "    return nnz * bytes_per_nnz / (1024**3)\n",
        "\n",
        "def compile_cuda_program():\n",
        "    subprocess.run([\"nvcc\", \"mainy.cu\", \"-O3\", \"-o\", \"mainy\"], check=True)\n",
        "\n",
        "def run_cuda_program(N, M):\n",
        "    with open(\"main.cu\", \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    content = content.replace(\"const int N = 1000;\", f\"const int N = {N};\")\n",
        "    content = content.replace(\"const int M = 1000;\", f\"const int M = {M};\")\n",
        "\n",
        "    thr = max(1, min(M, int(np.floor(M/3))))\n",
        "    content = content.replace(\"const int threshold = 20;\", f\"const int threshold = {thr};\")\n",
        "\n",
        "    with open(\"mainy.cu\", \"w\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    compile_cuda_program()\n",
        "    result = subprocess.run([\"./mainy\"], capture_output=True, text=True)\n",
        "    out = result.stdout\n",
        "    line = [l for l in out.splitlines() if \"CUDA kernel time:\" in l][0]\n",
        "    secs = float(line.split(\":\")[1].strip().split()[0])\n",
        "    return secs\n",
        "\n",
        "def create_sparse_matrix_and_vector(N, M, device):\n",
        "    # Build the same pattern as CUDA: (i+j)%3==0 ? i+j : 0\n",
        "    # Chunked to keep memory reasonable while constructing indices/values\n",
        "    chunk_rows = max(1, 10_000 // max(1, M))\n",
        "    idx_rows, idx_cols, vals = [], [], []\n",
        "\n",
        "    for r0 in range(0, N, chunk_rows):\n",
        "        r1 = min(N, r0 + chunk_rows)\n",
        "        for i in range(r0, r1):\n",
        "            # Positions j where (i+j)%3==0 -> j ≡ -i (mod 3)\n",
        "            start_j = (3 - (i % 3)) % 3\n",
        "            for j in range(start_j, M, 3):\n",
        "                idx_rows.append(i)\n",
        "                idx_cols.append(j)\n",
        "                vals.append(float(i + j))\n",
        "\n",
        "    indices = torch.tensor([idx_rows, idx_cols], dtype=torch.long, device=device)\n",
        "    values  = torch.tensor(vals, dtype=torch.float32, device=device)\n",
        "    A = torch.sparse_coo_tensor(indices, values, (N, M), device=device).coalesce()\n",
        "    X = torch.ones(M, 1, dtype=torch.float32, device=device)\n",
        "    return A, X\n",
        "\n",
        "def run_torch_program(N, M, iters=20):\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA GPU not available for PyTorch.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "    A, X = create_sparse_matrix_and_vector(N, M, device)\n",
        "\n",
        "    # Warmup\n",
        "    torch.cuda.synchronize()\n",
        "    _ = torch.sparse.mm(A, X)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    times_ms = []\n",
        "    for _ in range(iters):\n",
        "        torch.cuda.synchronize()\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end   = torch.cuda.Event(enable_timing=True)\n",
        "        start.record()\n",
        "        Y = torch.sparse.mm(A, X)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        times_ms.append(start.elapsed_time(end))\n",
        "    del A, X, Y\n",
        "    torch.cuda.empty_cache()\n",
        "    return float(np.mean(times_ms) / 1000.0)\n",
        "\n",
        "def main():\n",
        "    sizes = [(500,500), (1000,1000), (2000,2000), (3000,3000)]\n",
        "    used, total = get_memory_info_gb()\n",
        "    print(f\"System memory: used {used:.2f} GB / total {total:.2f} GB\")\n",
        "    print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "    labels, cuda_times, torch_times = [], [], []\n",
        "    for (N, M) in sizes:\n",
        "        est = estimate_memory_usage_gb(N, M)\n",
        "        safe = est <= total * 0.5   # skip if too big for sparse tensor construction\n",
        "        print(f\"\\nSize {N}x{M} | estimated sparse memory ~ {est:.2f} GB | safe: {safe}\")\n",
        "        if not safe:\n",
        "            print(\"Skipping PyTorch for this size due to memory estimate.\")\n",
        "        labels.append(f\"{N}x{M}\")\n",
        "\n",
        "        # CUDA (always runs – small dense arrays in CUDA code are on device)\n",
        "        try:\n",
        "            t_cuda = run_cuda_program(N, M)\n",
        "            cuda_times.append(t_cuda)\n",
        "            print(f\"Custom CUDA time: {t_cuda:.6f} s\")\n",
        "        except Exception as e:\n",
        "            print(f\"CUDA run failed: {e}\")\n",
        "            cuda_times.append(None)\n",
        "\n",
        "        # PyTorch sparse timing (only if safe)\n",
        "        if safe:\n",
        "            try:\n",
        "                t_torch = run_torch_program(N, M, iters=20)\n",
        "                torch_times.append(t_torch)\n",
        "                print(f\"PyTorch sparse time: {t_torch:.6f} s\")\n",
        "            except Exception as e:\n",
        "                print(f\"PyTorch run failed: {e}\")\n",
        "                torch_times.append(None)\n",
        "        else:\n",
        "            torch_times.append(None)\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Plot\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(labels, cuda_times, marker=\"o\", label=\"Custom CUDA (ELL+COO)\")\n",
        "    plt.plot(labels, torch_times, marker=\"s\", label=\"PyTorch sparse.mm\")\n",
        "    plt.xlabel(\"Matrix size\")\n",
        "    plt.ylabel(\"Time (s)\")\n",
        "    plt.title(\"SpMV performance comparison\")\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjZSz3R8_iGY",
        "outputId": "227eae3f-8737-499a-813a-bef515453100"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_benchmark.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install CUDA compiler (nvcc)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y cuda-toolkit-11-8\n",
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--tn-wYb-Pd7",
        "outputId": "c0754a4e-3103-4a76-ba76-4c5899a4a304"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package cuda-toolkit-11-8\n",
            "/bin/bash: line 1: nvcc: command not found\n"
          ]
        }
      ]
    }
  ]
}