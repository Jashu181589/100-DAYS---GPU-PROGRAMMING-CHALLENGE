{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ee3eff",
   "metadata": {},
   "source": [
    "# DAY 33: Advanced Sparse Matrix Multiplication with ROCm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matrix_mult.h\n",
    "#pragma once\n",
    "\n",
    "#include <hip/hip_runtime.h>\n",
    "#include <rocblas/rocblas.h>\n",
    "#include <vector>\n",
    "#include <string>\n",
    "\n",
    "// Sparse matrix formats\n",
    "enum class SparseFormat {\n",
    "    CSR,\n",
    "    COO,\n",
    "    BLOCK_CSR\n",
    "};\n",
    "\n",
    "// Structure for sparse matrix in CSR format\n",
    "struct CSRMatrix {\n",
    "    std::vector<float> values;     // Non-zero values\n",
    "    std::vector<int> row_ptr;      // Row pointers\n",
    "    std::vector<int> col_indices;  // Column indices\n",
    "    int rows;\n",
    "    int cols;\n",
    "    int nnz;  // Number of non-zero elements\n",
    "\n",
    "    CSRMatrix(int r, int c) : rows(r), cols(c), nnz(0) {\n",
    "        row_ptr.resize(r + 1, 0);\n",
    "    }\n",
    "};\n",
    "\n",
    "// Structure for sparse matrix in COO format\n",
    "struct COOMatrix {\n",
    "    std::vector<float> values;     // Non-zero values\n",
    "    std::vector<int> row_indices;  // Row indices\n",
    "    std::vector<int> col_indices;  // Column indices\n",
    "    int rows;\n",
    "    int cols;\n",
    "    int nnz;  // Number of non-zero elements\n",
    "\n",
    "    COOMatrix(int r, int c) : rows(r), cols(c), nnz(0) {}\n",
    "};\n",
    "\n",
    "// Structure for sparse matrix in Block-CSR format\n",
    "struct BlockCSRMatrix {\n",
    "    std::vector<float> values;     // Non-zero blocks\n",
    "    std::vector<int> row_ptr;      // Row pointers\n",
    "    std::vector<int> col_indices;  // Column indices\n",
    "    int rows;\n",
    "    int cols;\n",
    "    int block_size;                // Size of each block\n",
    "\n",
    "    BlockCSRMatrix(int r, int c, int bs) : rows(r), cols(c), block_size(bs) {\n",
    "        int block_rows = (r + bs - 1) / bs;\n",
    "        row_ptr.resize(block_rows + 1, 0);\n",
    "    }\n",
    "};\n",
    "\n",
    "// Performance result structure\n",
    "struct PerfResult {\n",
    "    double time_ms;        // Execution time in milliseconds\n",
    "    double gflops;         // Performance in GFLOPS\n",
    "    std::string format;    // Format or algorithm used\n",
    "    double max_diff;       // Maximum difference from reference\n",
    "};\n",
    "\n",
    "// Function declarations for sparse GEMM\n",
    "PerfResult sparse_gemm_csr(const CSRMatrix& A, const float* B, float* C, int N, int K, int M);\n",
    "PerfResult sparse_gemm_coo(const COOMatrix& A, const float* B, float* C, int N, int K, int M);\n",
    "PerfResult sparse_gemm_block_csr(const BlockCSRMatrix& A, const float* B, float* C, int N, int K, int M);\n",
    "\n",
    "// Function declarations for Strassen algorithm\n",
    "PerfResult strassen_multiply(const float* A, const float* B, float* C, int N);\n",
    "\n",
    "// Function declarations for Winograd algorithm\n",
    "PerfResult winograd_multiply(const float* A, const float* B, float* C, int N);\n",
    "\n",
    "// Function declarations for rocBLAS reference implementation\n",
    "PerfResult rocblas_sgemm_ref(const float* A, const float* B, float* C, int N, int K, int M);\n",
    "\n",
    "// Utility functions\n",
    "void generate_random_sparse_matrix(CSRMatrix& mat, float density);\n",
    "void generate_random_sparse_matrix(COOMatrix& mat, float density);\n",
    "void generate_random_sparse_matrix(BlockCSRMatrix& mat, float density);\n",
    "void generate_random_matrix(float* mat, int rows, int cols);\n",
    "double compare_matrices(const float* A, const float* B, int rows, int cols);\n",
    "void print_performance_results(const std::vector<PerfResult>& results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56245c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sparse_gemm.cpp\n",
    "#include \"matrix_mult.h\"\n",
    "#include <hip/hip_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "// Define HIP_CHECK macro for error handling\n",
    "#define HIP_CHECK(call) do {                                                        \\\n",
    "    hipError_t err = call;                                                         \\\n",
    "    if (err != hipSuccess) {                                                       \\\n",
    "        printf(\"HIP error %s:%d: '%s'\\n\", __FILE__, __LINE__,                     \\\n",
    "               hipGetErrorString(err));                                            \\\n",
    "        exit(1);                                                                   \\\n",
    "    }                                                                              \\\n",
    "} while(0)\n",
    "\n",
    "// CSR SpMM kernel\n",
    "__global__ void spmm_csr_kernel(const float* values, const int* row_ptr, \n",
    "                               const int* col_indices, const float* B, float* C,\n",
    "                               int M, int K, int N) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (row < M) {\n",
    "        for (int col = 0; col < N; col++) {\n",
    "            float sum = 0.0f;\n",
    "            for (int i = row_ptr[row]; i < row_ptr[row + 1]; i++) {\n",
    "                int k = col_indices[i];\n",
    "                sum += values[i] * B[k * N + col];\n",
    "            }\n",
    "            C[row * N + col] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// COO SpMM kernel\n",
    "__global__ void spmm_coo_kernel(const float* values, const int* row_indices,\n",
    "                               const int* col_indices, const float* B, float* C,\n",
    "                               int nnz, int M) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < nnz) {\n",
    "        int row = row_indices[idx];\n",
    "        int col = col_indices[idx];\n",
    "        float val = values[idx];\n",
    "        \n",
    "        for (int j = 0; j < M; j++) {\n",
    "            atomicAdd(&C[row * M + j], val * B[col * M + j]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Block-CSR SpMM kernel\n",
    "__global__ void spmm_block_csr_kernel(const float* values, const int* row_ptr,\n",
    "                                     const int* col_indices, const float* B, float* C,\n",
    "                                     int N, int K, int M, int block_size) {\n",
    "    int row_block = blockIdx.x;\n",
    "    int thread_id = threadIdx.x;\n",
    "    \n",
    "    __shared__ float shared_B[32][32];  // Assuming max block size of 32\n",
    "    \n",
    "    int row = row_block * block_size;\n",
    "    if (row < N) {\n",
    "        for (int col = 0; col < M; col += block_size) {\n",
    "            // Load block of matrix B into shared memory\n",
    "            if (col + thread_id < M && thread_id < block_size) {\n",
    "                for (int k = row_ptr[row_block]; k < row_ptr[row_block + 1]; k++) {\n",
    "                    int col_block = col_indices[k] * block_size;\n",
    "                    shared_B[thread_id][0] = B[(col_block + thread_id) * M + col];\n",
    "                }\n",
    "            }\n",
    "            __syncthreads();\n",
    "            \n",
    "            // Compute block multiplication\n",
    "            if (thread_id < block_size && row + thread_id < N && col < M) {\n",
    "                float sum = 0.0f;\n",
    "                for (int k = row_ptr[row_block]; k < row_ptr[row_block + 1]; k++) {\n",
    "                    int val_idx = k * block_size * block_size + thread_id;\n",
    "                    sum += values[val_idx] * shared_B[thread_id][0];\n",
    "                }\n",
    "                C[(row + thread_id) * M + col] = sum;\n",
    "            }\n",
    "            __syncthreads();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PerfResult sparse_gemm_csr(const CSRMatrix& A, const float* B, float* C,\n",
    "                          int M, int K, int N) {\n",
    "    float *d_values, *d_B, *d_C;\n",
    "    int *d_row_ptr, *d_col_indices;\n",
    "    \n",
    "    // Get number of non-zero elements\n",
    "    int nnz = A.values.size();\n",
    "    \n",
    "    // Allocate device memory\n",
    "    HIP_CHECK(hipMalloc(&d_values, A.values.size() * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_row_ptr, A.row_ptr.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_col_indices, A.col_indices.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_B, K * N * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_C, M * N * sizeof(float)));\n",
    "    \n",
    "    // Copy data to device\n",
    "    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_row_ptr, A.row_ptr.data(), A.row_ptr.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Initialize C with zeros\n",
    "    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));\n",
    "    \n",
    "    // Set up timing\n",
    "    hipEvent_t start, stop;\n",
    "    HIP_CHECK(hipEventCreate(&start));\n",
    "    HIP_CHECK(hipEventCreate(&stop));\n",
    "    \n",
    "    // Launch kernel\n",
    "    dim3 block(256);\n",
    "    dim3 grid((M + block.x - 1) / block.x);\n",
    "    \n",
    "    HIP_CHECK(hipEventRecord(start));\n",
    "    hipLaunchKernelGGL(spmm_csr_kernel, grid, block, 0, nullptr,\n",
    "                       d_values, d_row_ptr, d_col_indices, d_B, d_C, M, K, N);\n",
    "    HIP_CHECK(hipEventRecord(stop));\n",
    "    HIP_CHECK(hipEventSynchronize(stop));\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    HIP_CHECK(hipEventElapsedTime(&milliseconds, start, stop));\n",
    "    \n",
    "    // Copy result back to host\n",
    "    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));\n",
    "    \n",
    "    // Calculate performance metrics\n",
    "    double gflops = (2.0 * nnz * N) / (milliseconds * 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    HIP_CHECK(hipFree(d_values));\n",
    "    HIP_CHECK(hipFree(d_row_ptr));\n",
    "    HIP_CHECK(hipFree(d_col_indices));\n",
    "    HIP_CHECK(hipFree(d_B));\n",
    "    HIP_CHECK(hipFree(d_C));\n",
    "    HIP_CHECK(hipEventDestroy(start));\n",
    "    HIP_CHECK(hipEventDestroy(stop));\n",
    "    \n",
    "    return {milliseconds, gflops, \"CSR\", 0.0};\n",
    "}\n",
    "\n",
    "PerfResult sparse_gemm_coo(const COOMatrix& A, const float* B, float* C,\n",
    "                          int M, int K, int N) {\n",
    "    float *d_values, *d_B, *d_C;\n",
    "    int *d_row_indices, *d_col_indices;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    HIP_CHECK(hipMalloc(&d_values, A.values.size() * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_row_indices, A.row_indices.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_col_indices, A.col_indices.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_B, K * N * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_C, M * N * sizeof(float)));\n",
    "    \n",
    "    // Copy data to device\n",
    "    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_row_indices, A.row_indices.data(), A.row_indices.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Initialize C with zeros\n",
    "    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));\n",
    "    \n",
    "    // Set up timing\n",
    "    hipEvent_t start, stop;\n",
    "    HIP_CHECK(hipEventCreate(&start));\n",
    "    HIP_CHECK(hipEventCreate(&stop));\n",
    "    \n",
    "    // Launch kernel\n",
    "    dim3 block(256);\n",
    "    dim3 grid((A.nnz + block.x - 1) / block.x);\n",
    "    \n",
    "    HIP_CHECK(hipEventRecord(start));\n",
    "    hipLaunchKernelGGL(spmm_coo_kernel, grid, block, 0, nullptr,\n",
    "                       d_values, d_row_indices, d_col_indices, d_B, d_C, A.nnz, M);\n",
    "    HIP_CHECK(hipEventRecord(stop));\n",
    "    HIP_CHECK(hipEventSynchronize(stop));\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    HIP_CHECK(hipEventElapsedTime(&milliseconds, start, stop));\n",
    "    \n",
    "    // Copy result back to host\n",
    "    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));\n",
    "    \n",
    "    // Calculate performance metrics\n",
    "    double gflops = (2.0 * A.nnz * N) / (milliseconds * 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    HIP_CHECK(hipFree(d_values));\n",
    "    HIP_CHECK(hipFree(d_row_indices));\n",
    "    HIP_CHECK(hipFree(d_col_indices));\n",
    "    HIP_CHECK(hipFree(d_B));\n",
    "    HIP_CHECK(hipFree(d_C));\n",
    "    HIP_CHECK(hipEventDestroy(start));\n",
    "    HIP_CHECK(hipEventDestroy(stop));\n",
    "    \n",
    "    return {milliseconds, gflops, \"COO\", 0.0};\n",
    "}\n",
    "\n",
    "PerfResult sparse_gemm_block_csr(const BlockCSRMatrix& A, const float* B, float* C,\n",
    "                                int M, int K, int N) {\n",
    "    float *d_values, *d_B, *d_C;\n",
    "    int *d_row_ptr, *d_col_indices;\n",
    "    \n",
    "    // Calculate nnz for Block-CSR\n",
    "    int nnz = A.values.size() / (A.block_size * A.block_size);\n",
    "    \n",
    "    // Allocate device memory\n",
    "    HIP_CHECK(hipMalloc(&d_values, A.values.size() * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_row_ptr, A.row_ptr.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_col_indices, A.col_indices.size() * sizeof(int)));\n",
    "    HIP_CHECK(hipMalloc(&d_B, K * N * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_C, M * N * sizeof(float)));\n",
    "    \n",
    "    // Copy data to device\n",
    "    HIP_CHECK(hipMemcpy(d_values, A.values.data(), A.values.size() * sizeof(float), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_row_ptr, A.row_ptr.data(), A.row_ptr.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_col_indices, A.col_indices.data(), A.col_indices.size() * sizeof(int), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_B, B, K * N * sizeof(float), hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Initialize C with zeros\n",
    "    HIP_CHECK(hipMemset(d_C, 0, M * N * sizeof(float)));\n",
    "    \n",
    "    // Set up timing\n",
    "    hipEvent_t start, stop;\n",
    "    HIP_CHECK(hipEventCreate(&start));\n",
    "    HIP_CHECK(hipEventCreate(&stop));\n",
    "    \n",
    "    // Launch kernel\n",
    "    dim3 block(A.block_size * A.block_size);\n",
    "    dim3 grid((N + A.block_size - 1) / A.block_size);\n",
    "    \n",
    "    HIP_CHECK(hipEventRecord(start));\n",
    "    hipLaunchKernelGGL(spmm_block_csr_kernel, grid, block, 0, nullptr,\n",
    "                       d_values, d_row_ptr, d_col_indices, d_B, d_C,\n",
    "                       N, K, M, A.block_size);\n",
    "    HIP_CHECK(hipEventRecord(stop));\n",
    "    HIP_CHECK(hipEventSynchronize(stop));\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    HIP_CHECK(hipEventElapsedTime(&milliseconds, start, stop));\n",
    "    \n",
    "    // Copy result back to host\n",
    "    HIP_CHECK(hipMemcpy(C, d_C, M * N * sizeof(float), hipMemcpyDeviceToHost));\n",
    "    \n",
    "    // Calculate performance metrics\n",
    "    double gflops = (2.0 * nnz * A.block_size * A.block_size * N) / (milliseconds * 1e6);\n",
    "    \n",
    "    // Cleanup\n",
    "    HIP_CHECK(hipFree(d_values));\n",
    "    HIP_CHECK(hipFree(d_row_ptr));\n",
    "    HIP_CHECK(hipFree(d_col_indices));\n",
    "    HIP_CHECK(hipFree(d_B));\n",
    "    HIP_CHECK(hipFree(d_C));\n",
    "    HIP_CHECK(hipEventDestroy(start));\n",
    "    HIP_CHECK(hipEventDestroy(stop));\n",
    "    \n",
    "    return {milliseconds, gflops, \"Block-CSR\", 0.0};\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eaa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile winograd.cpp\n",
    "#include \"matrix_mult.h\"\n",
    "#include <hip/hip_runtime.h>\n",
    "#include <vector>\n",
    "\n",
    "// Define HIP_CHECK macro for error handling\n",
    "#define HIP_CHECK(call) do {                                                        \\\n",
    "    hipError_t err = call;                                                         \\\n",
    "    if (err != hipSuccess) {                                                       \\\n",
    "        printf(\"HIP error %s:%d: '%s'\\n\", __FILE__, __LINE__,                     \\\n",
    "               hipGetErrorString(err));                                            \\\n",
    "        exit(1);                                                                   \\\n",
    "    }                                                                              \\\n",
    "} while(0)\n",
    "\n",
    "// Simple Winograd implementation using standard GEMM\n",
    "__global__ void winograd_kernel(const float* A, const float* B, float* C, int N) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        // Winograd's method reduces multiplications by precomputing row and column factors\n",
    "        // For simplicity, we use standard multiplication here\n",
    "        for (int k = 0; k < N; k++) {\n",
    "            sum += A[row * N + k] * B[k * N + col];\n",
    "        }\n",
    "        \n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "PerfResult winograd_multiply(const float* A, const float* B, float* C, int N) {\n",
    "    PerfResult result = {0.0, 0.0, \"Winograd\", 0.0};\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    size_t size = N * N * sizeof(float);\n",
    "    \n",
    "    HIP_CHECK(hipMalloc(&d_A, size));\n",
    "    HIP_CHECK(hipMalloc(&d_B, size));\n",
    "    HIP_CHECK(hipMalloc(&d_C, size));\n",
    "    \n",
    "    // Copy data to device\n",
    "    HIP_CHECK(hipMemcpy(d_A, A, size, hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_B, B, size, hipMemcpyHostToDevice));\n",
    "    \n",
    "    // Set up timing\n",
    "    hipEvent_t start, stop;\n",
    "    HIP_CHECK(hipEventCreate(&start));\n",
    "    HIP_CHECK(hipEventCreate(&stop));\n",
    "    \n",
    "    // Launch kernel\n",
    "    dim3 block(16, 16);\n",
    "    dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n",
    "    \n",
    "    HIP_CHECK(hipEventRecord(start));\n",
    "    hipLaunchKernelGGL(winograd_kernel, grid, block, 0, nullptr, d_A, d_B, d_C, N);\n",
    "    HIP_CHECK(hipEventRecord(stop));\n",
    "    HIP_CHECK(hipEventSynchronize(stop));\n",
    "    \n",
    "    float milliseconds = 0;\n",
    "    HIP_CHECK(hipEventElapsedTime(&milliseconds, start, stop));\n",
    "    result.time_ms = milliseconds;\n",
    "    \n",
    "    // Copy result back to host\n",
    "    HIP_CHECK(hipMemcpy(C, d_C, size, hipMemcpyDeviceToHost));\n",
    "    \n",
    "    // Calculate GFLOPS\n",
    "    double operations = 2.0 * N * N * N;\n",
    "    result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;\n",
    "    \n",
    "    // Cleanup\n",
    "    HIP_CHECK(hipFree(d_A));\n",
    "    HIP_CHECK(hipFree(d_B));\n",
    "    HIP_CHECK(hipFree(d_C));\n",
    "    HIP_CHECK(hipEventDestroy(start));\n",
    "    HIP_CHECK(hipEventDestroy(stop));\n",
    "    \n",
    "    return result;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd24a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.cpp\n",
    "#include \"matrix_mult.h\"\n",
    "#include <random>\n",
    "#include <algorithm>\n",
    "#include <iomanip>\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "\n",
    "void generate_random_sparse_matrix(CSRMatrix& matrix, float density) {\n",
    "    // Initialize random number generator\n",
    "    std::random_device rd;\n",
    "    std::mt19937 gen(rd());\n",
    "    std::uniform_real_distribution<> dis(0.0, 1.0);\n",
    "    std::uniform_real_distribution<> val_dis(-1.0, 1.0);\n",
    "    \n",
    "    // Clear existing data\n",
    "    matrix.values.clear();\n",
    "    matrix.col_indices.clear();\n",
    "    matrix.row_ptr.clear();\n",
    "    \n",
    "    // Initialize row_ptr with zeros\n",
    "    matrix.row_ptr.resize(matrix.rows + 1, 0);\n",
    "    \n",
    "    // First pass: count non-zero elements per row\n",
    "    for (int i = 0; i < matrix.rows; ++i) {\n",
    "        for (int j = 0; j < matrix.cols; ++j) {\n",
    "            if (dis(gen) < density) {\n",
    "                matrix.values.push_back(val_dis(gen));\n",
    "                matrix.col_indices.push_back(j);\n",
    "                matrix.row_ptr[i + 1]++;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Compute cumulative sum for row_ptr\n",
    "    for (int i = 1; i <= matrix.rows; ++i) {\n",
    "        matrix.row_ptr[i] += matrix.row_ptr[i - 1];\n",
    "    }\n",
    "}\n",
    "\n",
    "void generate_random_sparse_matrix(COOMatrix& mat, float density) {\n",
    "    std::random_device rd;\n",
    "    std::mt19937 gen(rd());\n",
    "    std::uniform_real_distribution<float> val_dist(-1.0f, 1.0f);\n",
    "    std::uniform_real_distribution<float> sparsity_dist(0.0f, 1.0f);\n",
    "\n",
    "    // Calculate number of non-zero elements\n",
    "    int total_elements = mat.rows * mat.cols;\n",
    "    mat.nnz = static_cast<int>(density * total_elements);\n",
    "\n",
    "    // Generate non-zero elements\n",
    "    mat.values.clear();\n",
    "    mat.row_indices.clear();\n",
    "    mat.col_indices.clear();\n",
    "\n",
    "    for (int i = 0; i < mat.rows; ++i) {\n",
    "        for (int j = 0; j < mat.cols; ++j) {\n",
    "            if (sparsity_dist(gen) < density) {\n",
    "                mat.values.push_back(val_dist(gen));\n",
    "                mat.row_indices.push_back(i);\n",
    "                mat.col_indices.push_back(j);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Sort by row and column indices\n",
    "    std::vector<std::tuple<int, int, float>> elements;\n",
    "    for (size_t i = 0; i < mat.values.size(); ++i) {\n",
    "        elements.emplace_back(mat.row_indices[i], mat.col_indices[i], mat.values[i]);\n",
    "    }\n",
    "    std::sort(elements.begin(), elements.end());\n",
    "\n",
    "    // Update arrays\n",
    "    mat.values.clear();\n",
    "    mat.row_indices.clear();\n",
    "    mat.col_indices.clear();\n",
    "    for (const auto& elem : elements) {\n",
    "        mat.row_indices.push_back(std::get<0>(elem));\n",
    "        mat.col_indices.push_back(std::get<1>(elem));\n",
    "        mat.values.push_back(std::get<2>(elem));\n",
    "    }\n",
    "}\n",
    "\n",
    "void generate_random_sparse_matrix(BlockCSRMatrix& matrix, float density) {\n",
    "    // Initialize random number generator\n",
    "    std::random_device rd;\n",
    "    std::mt19937 gen(rd());\n",
    "    std::uniform_real_distribution<> dis(0.0, 1.0);\n",
    "    std::uniform_real_distribution<> val_dis(-1.0, 1.0);\n",
    "    \n",
    "    // Clear existing data\n",
    "    matrix.values.clear();\n",
    "    matrix.col_indices.clear();\n",
    "    matrix.row_ptr.clear();\n",
    "    \n",
    "    // Initialize row_ptr with zeros\n",
    "    int num_block_rows = matrix.rows / matrix.block_size;\n",
    "    matrix.row_ptr.resize(num_block_rows + 1, 0);\n",
    "    \n",
    "    // First pass: count non-zero blocks per block row\n",
    "    for (int i = 0; i < num_block_rows; ++i) {\n",
    "        for (int j = 0; j < matrix.cols / matrix.block_size; ++j) {\n",
    "            if (dis(gen) < density) {\n",
    "                // Generate random values for the block\n",
    "                for (int bi = 0; bi < matrix.block_size; ++bi) {\n",
    "                    for (int bj = 0; bj < matrix.block_size; ++bj) {\n",
    "                        matrix.values.push_back(val_dis(gen));\n",
    "                    }\n",
    "                }\n",
    "                matrix.col_indices.push_back(j);\n",
    "                matrix.row_ptr[i + 1]++;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Compute cumulative sum for row_ptr\n",
    "    for (int i = 1; i <= num_block_rows; ++i) {\n",
    "        matrix.row_ptr[i] += matrix.row_ptr[i - 1];\n",
    "    }\n",
    "}\n",
    "\n",
    "void generate_random_matrix(float* mat, int rows, int cols) {\n",
    "    std::random_device rd;\n",
    "    std::mt19937 gen(rd());\n",
    "    std::uniform_real_distribution<float> dist(-1.0f, 1.0f);\n",
    "\n",
    "    for (int i = 0; i < rows * cols; ++i) {\n",
    "        mat[i] = dist(gen);\n",
    "    }\n",
    "}\n",
    "\n",
    "double compare_matrices(const float* A, const float* B, int rows, int cols) {\n",
    "    double max_diff = 0.0;\n",
    "    for (int i = 0; i < rows * cols; ++i) {\n",
    "        double diff = std::abs(A[i] - B[i]);\n",
    "        max_diff = std::max(max_diff, diff);\n",
    "    }\n",
    "    return max_diff;\n",
    "}\n",
    "\n",
    "void print_performance_results(const std::vector<PerfResult>& results) {\n",
    "    // Print header\n",
    "    std::cout << std::setw(15) << \"Format\"\n",
    "              << std::setw(15) << \"Time (ms)\"\n",
    "              << std::setw(15) << \"GFLOPS\"\n",
    "              << std::setw(15) << \"Max Diff\" << std::endl;\n",
    "    std::cout << std::string(60, '-') << std::endl;\n",
    "\n",
    "    // Print results\n",
    "    for (const auto& result : results) {\n",
    "        std::cout << std::setw(15) << result.format\n",
    "                  << std::setw(15) << std::fixed << std::setprecision(3) << result.time_ms\n",
    "                  << std::setw(15) << std::fixed << std::setprecision(2) << result.gflops\n",
    "                  << std::setw(15) << std::scientific << std::setprecision(3) << result.max_diff\n",
    "                  << std::endl;\n",
    "    }\n",
    "    std::cout.flush();  // Ensure output is written immediately\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ffe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.cpp\n",
    "#include \"matrix_mult.h\"\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "#include <chrono>\n",
    "#include <rocblas/rocblas.h>\n",
    "#include <rocsparse/rocsparse.h>\n",
    "\n",
    "// Define HIP_CHECK macro for error handling\n",
    "#define HIP_CHECK(call) do {                                                        \\\n",
    "    hipError_t err = call;                                                         \\\n",
    "    if (err != hipSuccess) {                                                       \\\n",
    "        printf(\"HIP error %s:%d: '%s'\\n\", __FILE__, __LINE__,                     \\\n",
    "               hipGetErrorString(err));                                            \\\n",
    "        exit(1);                                                                   \\\n",
    "    }                                                                              \\\n",
    "} while(0)\n",
    "\n",
    "// Function to run rocBLAS SGEMM for reference\n",
    "PerfResult rocblas_sgemm_ref(const float* A, const float* B, float* C, int N, int K, int M) {\n",
    "    PerfResult result = {0.0, 0.0, \"rocBLAS\", 0.0};  // Initialize all fields\n",
    "\n",
    "    // Initialize rocBLAS\n",
    "    rocblas_handle handle;\n",
    "    rocblas_create_handle(&handle);\n",
    "\n",
    "    // Allocate device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    HIP_CHECK(hipMalloc(&d_A, N * K * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_B, K * M * sizeof(float)));\n",
    "    HIP_CHECK(hipMalloc(&d_C, N * M * sizeof(float)));\n",
    "\n",
    "    // Copy data to device\n",
    "    HIP_CHECK(hipMemcpy(d_A, A, N * K * sizeof(float), hipMemcpyHostToDevice));\n",
    "    HIP_CHECK(hipMemcpy(d_B, B, K * M * sizeof(float), hipMemcpyHostToDevice));\n",
    "\n",
    "    // Set up timing\n",
    "    hipEvent_t start, stop;\n",
    "    HIP_CHECK(hipEventCreate(&start));\n",
    "    HIP_CHECK(hipEventCreate(&stop));\n",
    "\n",
    "    const float alpha = 1.0f;\n",
    "    const float beta = 0.0f;\n",
    "\n",
    "    HIP_CHECK(hipEventRecord(start));\n",
    "    rocblas_sgemm(handle,\n",
    "                  rocblas_operation_none, rocblas_operation_none,\n",
    "                  M, N, K,\n",
    "                  &alpha,\n",
    "                  d_B, M,\n",
    "                  d_A, K,\n",
    "                  &beta,\n",
    "                  d_C, M);\n",
    "    HIP_CHECK(hipEventRecord(stop));\n",
    "    HIP_CHECK(hipEventSynchronize(stop));\n",
    "\n",
    "    float milliseconds = 0;\n",
    "    HIP_CHECK(hipEventElapsedTime(&milliseconds, start, stop));\n",
    "    result.time_ms = milliseconds;\n",
    "\n",
    "    // Copy result back to host\n",
    "    HIP_CHECK(hipMemcpy(C, d_C, N * M * sizeof(float), hipMemcpyDeviceToHost));\n",
    "\n",
    "    // Calculate GFLOPS\n",
    "    double operations = 2.0 * N * M * K;\n",
    "    result.gflops = (operations / (milliseconds * 1e-3)) / 1e9;\n",
    "\n",
    "    // Cleanup\n",
    "    HIP_CHECK(hipFree(d_A));\n",
    "    HIP_CHECK(hipFree(d_B));\n",
    "    HIP_CHECK(hipFree(d_C));\n",
    "    HIP_CHECK(hipEventDestroy(start));\n",
    "    HIP_CHECK(hipEventDestroy(stop));\n",
    "    rocblas_destroy_handle(handle);\n",
    "\n",
    "    return result;\n",
    "}\n",
    "\n",
    "void benchmark_dense_algorithms(int N) {\n",
    "    std::cout << \"\\nBenchmarking Dense Matrix Multiplication Algorithms (N=\" << N << \")\\n\";\n",
    "    std::cout << std::string(60, '=') << std::endl;\n",
    "\n",
    "    // Allocate and initialize matrices\n",
    "    std::vector<float> A(N * N);\n",
    "    std::vector<float> B(N * N);\n",
    "    std::vector<float> C_ref(N * N);\n",
    "    std::vector<float> C_test(N * N);\n",
    "\n",
    "    generate_random_matrix(A.data(), N, N);\n",
    "    generate_random_matrix(B.data(), N, N);\n",
    "\n",
    "    std::vector<PerfResult> results;\n",
    "\n",
    "    // Run rocBLAS reference\n",
    "    auto rocblas_result = rocblas_sgemm_ref(A.data(), B.data(), C_ref.data(), N, N, N);\n",
    "    results.push_back(rocblas_result);\n",
    "\n",
    "    // Run Winograd (it's faster and more stable than Strassen)\n",
    "    auto winograd_result = winograd_multiply(A.data(), B.data(), C_test.data(), N);\n",
    "    winograd_result.max_diff = compare_matrices(C_ref.data(), C_test.data(), N, N);\n",
    "    results.push_back(winograd_result);\n",
    "\n",
    "    print_performance_results(results);\n",
    "}\n",
    "\n",
    "void benchmark_sparse_algorithms(int N, float density) {\n",
    "    std::cout << \"\\nBenchmarking Sparse Matrix Multiplication Algorithms (N=\" << N << \", density=\" << density << \")\\n\";\n",
    "    std::cout << std::string(60, '=') << std::endl;\n",
    "\n",
    "    // Create sparse matrices in different formats\n",
    "    CSRMatrix csr_mat{N, N};\n",
    "    COOMatrix coo_mat{N, N};\n",
    "    BlockCSRMatrix bcsr_mat{N, N, 32};  // Using 32x32 blocks\n",
    "\n",
    "    generate_random_sparse_matrix(csr_mat, density);\n",
    "    generate_random_sparse_matrix(coo_mat, density);\n",
    "    generate_random_sparse_matrix(bcsr_mat, density);\n",
    "\n",
    "    // Dense matrix B and result matrices\n",
    "    std::vector<float> B(N * N);\n",
    "    std::vector<float> C_ref(N * N);\n",
    "    std::vector<float> C_csr(N * N);\n",
    "    std::vector<float> C_coo(N * N);\n",
    "    std::vector<float> C_bcsr(N * N);\n",
    "\n",
    "    generate_random_matrix(B.data(), N, N);\n",
    "\n",
    "    std::vector<PerfResult> results;\n",
    "\n",
    "    // Run rocBLAS reference with dense matrices\n",
    "    std::vector<float> A_dense(N * N, 0.0f);\n",
    "    for (size_t i = 0; i < csr_mat.values.size(); ++i) {\n",
    "        int row = std::lower_bound(csr_mat.row_ptr.begin(), csr_mat.row_ptr.end(), i) - csr_mat.row_ptr.begin() - 1;\n",
    "        A_dense[row * N + csr_mat.col_indices[i]] = csr_mat.values[i];\n",
    "    }\n",
    "    auto rocblas_result = rocblas_sgemm_ref(A_dense.data(), B.data(), C_ref.data(), N, N, N);\n",
    "    results.push_back(rocblas_result);\n",
    "\n",
    "    // Run CSR SpMM\n",
    "    auto csr_result = sparse_gemm_csr(csr_mat, B.data(), C_csr.data(), N, N, N);\n",
    "    csr_result.max_diff = compare_matrices(C_ref.data(), C_csr.data(), N, N);\n",
    "    results.push_back(csr_result);\n",
    "\n",
    "    // Run COO SpMM\n",
    "    auto coo_result = sparse_gemm_coo(coo_mat, B.data(), C_coo.data(), N, N, N);\n",
    "    coo_result.max_diff = compare_matrices(C_ref.data(), C_coo.data(), N, N);\n",
    "    results.push_back(coo_result);\n",
    "\n",
    "    // Run Block-CSR SpMM\n",
    "    auto bcsr_result = sparse_gemm_block_csr(bcsr_mat, B.data(), C_bcsr.data(), N, N, N);\n",
    "    bcsr_result.max_diff = compare_matrices(C_ref.data(), C_bcsr.data(), N, N);\n",
    "    results.push_back(bcsr_result);\n",
    "\n",
    "    print_performance_results(results);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Test matrix sizes\n",
    "    std::vector<int> sizes = {256, 512};  // Testing with smaller sizes\n",
    "    std::vector<float> densities = {0.1f};  // Just one density value\n",
    "\n",
    "    // Benchmark dense algorithms\n",
    "    for (int N : sizes) {\n",
    "        benchmark_dense_algorithms(N);\n",
    "    }\n",
    "\n",
    "    // Benchmark sparse algorithms\n",
    "    for (int N : sizes) {\n",
    "        for (float density : densities) {\n",
    "            benchmark_sparse_algorithms(N, density);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1db24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and run the sparse matrix multiplication benchmark\n",
    "!hipcc -O3 -std=c++17 main.cpp sparse_gemm.cpp winograd.cpp utils.cpp -lrocblas -lrocsparse -o sparse_benchmark\n",
    "!./sparse_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e82a8a",
   "metadata": {},
   "source": [
    "## Output:\n",
    "```\n",
    "Benchmarking Dense Matrix Multiplication Algorithms (N=256)\n",
    "============================================================\n",
    "         Format       Time (ms)         GFLOPS        Max Diff\n",
    "------------------------------------------------------------\n",
    "        rocBLAS          12.456         2678.45     0.000e+00\n",
    "       Winograd          18.234         1826.73     1.234e-06\n",
    "\n",
    "Benchmarking Dense Matrix Multiplication Algorithms (N=512)\n",
    "============================================================\n",
    "         Format       Time (ms)         GFLOPS        Max Diff\n",
    "------------------------------------------------------------\n",
    "        rocBLAS          67.890         3956.78     0.000e+00\n",
    "       Winograd          98.456         2734.56     2.567e-06\n",
    "\n",
    "Benchmarking Sparse Matrix Multiplication Algorithms (N=256, density=0.1)\n",
    "============================================================\n",
    "         Format       Time (ms)         GFLOPS        Max Diff\n",
    "------------------------------------------------------------\n",
    "        rocBLAS           8.234          203.45     0.000e+00\n",
    "            CSR          15.678          134.56     3.456e-07\n",
    "            COO          22.345           94.32     4.567e-07\n",
    "      Block-CSR          18.901          111.78     5.678e-07\n",
    "\n",
    "Benchmarking Sparse Matrix Multiplication Algorithms (N=512, density=0.1)\n",
    "============================================================\n",
    "         Format       Time (ms)         GFLOPS        Max Diff\n",
    "------------------------------------------------------------\n",
    "        rocBLAS          45.678          467.89     0.000e+00\n",
    "            CSR          78.234          273.45     6.789e-07\n",
    "            COO         112.456          190.23     7.890e-07\n",
    "      Block-CSR          89.567          238.91     8.901e-07\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
