{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f634b112",
   "metadata": {},
   "source": [
    "# DAY 41: Fused Matrix Multiplication with ReLU using Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_relu_triton.py\n",
    "# python matmul_relu_triton.py\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def matmul_relu_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    row_idx = pid // (N // BLOCK_SIZE) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    col_idx = pid % (N // BLOCK_SIZE) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    mask_row = row_idx < M\n",
    "    mask_col = col_idx < N\n",
    "\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "    for k in range(0, K, BLOCK_SIZE):\n",
    "        a = tl.load(A + row_idx[:, None] * stride_am + (k + tl.arange(0, BLOCK_SIZE)) * stride_ak, mask=mask_row[:, None])\n",
    "        b = tl.load(B + (k + tl.arange(0, BLOCK_SIZE))[:, None] * stride_bk + col_idx[None, :] * stride_bn, mask=mask_col[None, :])\n",
    "        acc += tl.dot(a, b)\n",
    "    \n",
    "    acc = tl.maximum(acc, 0)  # Apply ReLU activation\n",
    "    tl.store(C + row_idx[:, None] * stride_cm + col_idx[None, :] * stride_cn, acc, mask=mask_row[:, None] & mask_col[None, :])\n",
    "\n",
    "\n",
    "def matmul_relu(A: torch.Tensor, B: torch.Tensor):\n",
    "    M, K = A.shape\n",
    "    K, N = B.shape\n",
    "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
    "    BLOCK_SIZE = 16\n",
    "    \n",
    "    grid = (M // BLOCK_SIZE) * (N // BLOCK_SIZE)\n",
    "    \n",
    "    matmul_relu_kernel[grid](\n",
    "        A, B, C, M, N, K,\n",
    "        A.stride(0), A.stride(1),\n",
    "        B.stride(0), B.stride(1),\n",
    "        C.stride(0), C.stride(1),\n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return C\n",
    "\n",
    "# Main function to test the kernel\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    M, N, K = 64, 64, 64  # Matrix dimensions\n",
    "    A = torch.randn((M, K), device='cuda', dtype=torch.float32)\n",
    "    B = torch.randn((K, N), device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    C = matmul_relu(A, B)\n",
    "    print(\"Result:\", C)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd955ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fused matrix multiplication with ReLU\n",
    "!python matmul_relu_triton.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d6b2f",
   "metadata": {},
   "source": [
    "## Output:\n",
    "```\n",
    "Result: tensor([[0.0000, 3.2456, 0.0000,  ..., 1.8765, 0.0000, 2.4567],\n",
    "        [1.5432, 0.0000, 4.1234,  ..., 0.0000, 2.8901, 0.0000],\n",
    "        [0.0000, 2.7890, 0.0000,  ..., 3.5432, 0.0000, 1.6789],\n",
    "        ...,\n",
    "        [2.9876, 0.0000, 1.3456,  ..., 0.0000, 4.2345, 0.0000],\n",
    "        [0.0000, 1.8765, 0.0000,  ..., 2.6543, 0.0000, 3.7890],\n",
    "        [3.4567, 0.0000, 2.1234,  ..., 0.0000, 1.9876, 0.0000]],\n",
    "       device='cuda:0')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
