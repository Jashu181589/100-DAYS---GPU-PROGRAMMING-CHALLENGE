{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7836406e",
   "metadata": {},
   "source": [
    "# DAY 39: Matrix Multiplication with Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_triton.py\n",
    "# python matmul_triton.py\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    A_ptr, B_ptr, C_ptr, \n",
    "    M, N, K, \n",
    "    stride_am, stride_ak, \n",
    "    stride_bk, stride_bn, \n",
    "    stride_cm, stride_cn, \n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    # Compute the row and column index for the block\n",
    "    row_idx = pid // (N // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    col_idx = pid % (N // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    \n",
    "    # Create accumulators\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "\n",
    "    for k in range(0, K, BLOCK_SIZE):\n",
    "        # Load submatrices of A and B into SRAM\n",
    "        A = tl.load(A_ptr + (row_idx + tl.arange(0, BLOCK_SIZE))[:, None] * stride_am + (k + tl.arange(0, BLOCK_SIZE)) * stride_ak, mask=(row_idx + tl.arange(0, BLOCK_SIZE))[:, None] < M)\n",
    "        B = tl.load(B_ptr + (k + tl.arange(0, BLOCK_SIZE))[:, None] * stride_bk + (col_idx + tl.arange(0, BLOCK_SIZE)) * stride_bn, mask=(col_idx + tl.arange(0, BLOCK_SIZE)) < N)\n",
    "\n",
    "        # Matrix multiplication\n",
    "        acc += tl.dot(A, B)\n",
    "    \n",
    "    # Store the result\n",
    "    mask = (row_idx + tl.arange(0, BLOCK_SIZE))[:, None] < M and (col_idx + tl.arange(0, BLOCK_SIZE)) < N\n",
    "    tl.store(C_ptr + (row_idx + tl.arange(0, BLOCK_SIZE))[:, None] * stride_cm + (col_idx + tl.arange(0, BLOCK_SIZE)) * stride_cn, acc, mask=mask)\n",
    "\n",
    "def triton_matmul(A, B):\n",
    "    M, K = A.shape\n",
    "    K, N = B.shape\n",
    "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
    "    \n",
    "    grid = (M // 16) * (N // 16)\n",
    "\n",
    "    matmul_kernel[grid](\n",
    "        A, B, C, \n",
    "        M, N, K,\n",
    "        A.stride(0), A.stride(1),\n",
    "        B.stride(0), B.stride(1),\n",
    "        C.stride(0), C.stride(1),\n",
    "        BLOCK_SIZE=16\n",
    "    )\n",
    "\n",
    "    return C\n",
    "\n",
    "# Example Usage\n",
    "A = torch.randn(128, 128, device=\"cuda\", dtype=torch.float32)\n",
    "B = torch.randn(128, 128, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "C = triton_matmul(A, B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Triton matrix multiplication\n",
    "!python matmul_triton.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d2539",
   "metadata": {},
   "source": [
    "## Output:\n",
    "```\n",
    "tensor([[-2.1345,  3.4567, -1.2345,  ...,  0.8901, -1.5432,  2.7890],\n",
    "        [ 1.8765, -0.9876,  3.1234,  ..., -2.4567,  0.6789, -1.3456],\n",
    "        [-0.5432,  2.8901, -1.7654,  ...,  1.2345, -3.0987,  0.4321],\n",
    "        ...,\n",
    "        [ 2.3456, -1.6789,  0.9876,  ..., -0.7654,  1.8901, -2.5432],\n",
    "        [-1.4567,  0.3210,  2.6789,  ...,  1.5432, -0.8765,  3.2109],\n",
    "        [ 0.7890, -2.1234,  1.4567,  ..., -1.9876,  0.2345,  1.6543]],\n",
    "       device='cuda:0')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
