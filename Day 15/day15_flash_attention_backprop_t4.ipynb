{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfqH8kl2WoI6"
      },
      "source": [
        "# Flash Attention Backpropagation\n",
        "**Project Structure:**\n",
        "- `flash.cu` - Main Flash Attention implementation\n",
        "- `helper.cu` - Helper functions and utilities\n",
        "- `helper.cuh` - Header file with declarations\n",
        "- `kernels.cu` - CUDA kernels for convolution and attention\n",
        "- `kernels.cuh` - Kernel declarations and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPGi8Q8OWoI8",
        "outputId": "3e0d96ca-e4dd-4c5a-b055-a61873ad7338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 23 05:16:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "============================================================\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# Check T4 GPU availability\n",
        "!nvidia-smi\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43kErJQ7WoI9",
        "outputId": "3b19e6eb-7a85-40c7-b43b-385ce6b639fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created kernels.cuh\n"
          ]
        }
      ],
      "source": [
        "# Create kernels.cuh - Header file with kernel declarations and T4 constants\n",
        "kernels_cuh = '''\n",
        "#ifndef KERNELS_CUH\n",
        "#define KERNELS_CUH\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath>\n",
        "\n",
        "// T4 GPU optimized constants\n",
        "#define CUDA_MAX_NUM_THREADS 1024\n",
        "#define BLOCK_SIZE 256\n",
        "#define WARP_SIZE 32\n",
        "#define T4_SM_COUNT 40\n",
        "#define T4_SHARED_MEM_SIZE 65536  // 64KB per SM\n",
        "#define T4_MAX_THREADS_PER_BLOCK 1024\n",
        "\n",
        "// Error checking macro\n",
        "#define CUDA_CHECK(call) do { \\\\\n",
        "    cudaError_t err = call; \\\\\n",
        "    if (err != cudaSuccess) { \\\\\n",
        "        printf(\"CUDA error at %s:%d - %s\\\\n\", __FILE__, __LINE__, cudaGetErrorString(err)); \\\\\n",
        "        exit(1); \\\\\n",
        "    } \\\\\n",
        "} while(0)\n",
        "\n",
        "// Convolution kernel declarations\n",
        "__global__ void unrollKernel(const float* input, float* input_unrolled,\n",
        "                            const int input_channels, const int input_height, const int input_width,\n",
        "                            const int kernel_size, const int output_height, const int output_width);\n",
        "\n",
        "__global__ void convolutionKernel(const float* input_unrolled, const float* weights, float* output,\n",
        "                                 const int output_size, const int num_filters, const int filter_size);\n",
        "\n",
        "__global__ void convolutionKernelOptimized(const float* input_unrolled, const float* weights, float* output,\n",
        "                                          const int output_size, const int num_filters, const int filter_size);\n",
        "\n",
        "// Backward pass kernels\n",
        "template <typename T>\n",
        "__global__ void compute_dLdW(T* dLdY, T* input_unrolled, T* dLdW,\n",
        "                            int output_height, int output_width, int num_filters, int filter_size);\n",
        "\n",
        "template <typename T>\n",
        "__global__ void compute_dLdX(T* dLdY, T* weights, T* dLdX_unrolled,\n",
        "                            int output_height, int output_width, int num_filters, int filter_size);\n",
        "\n",
        "// Attention kernels\n",
        "__global__ void flashAttentionForward(const float* Q, const float* K, const float* V,\n",
        "                                     float* output, float* l, float* m,\n",
        "                                     int batch_size, int seq_len, int head_dim, int num_heads);\n",
        "\n",
        "__global__ void flashAttentionBackward(const float* Q, const float* K, const float* V,\n",
        "                                      const float* dO, float* dQ, float* dK, float* dV,\n",
        "                                      const float* l, const float* m,\n",
        "                                      int batch_size, int seq_len, int head_dim, int num_heads);\n",
        "\n",
        "// Pooling kernels\n",
        "__global__ void maxPoolingKernel(float* input, float* output,\n",
        "                               int input_height, int input_width, int pool_size, int stride);\n",
        "\n",
        "template <typename T>\n",
        "__global__ void maxPoolingBackwardKernel(T* dLdY, T* input, T* dLdX,\n",
        "                                        int input_height, int input_width, int pool_size, int stride);\n",
        "\n",
        "#endif // KERNELS_CUH\n",
        "'''\n",
        "\n",
        "with open('kernels.cuh', 'w') as f:\n",
        "    f.write(kernels_cuh)\n",
        "\n",
        "print(\"Created kernels.cuh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRnsWUiRWoI-",
        "outputId": "303cbc58-c259-4b4b-919b-4076ad576a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created helper.cuh\n"
          ]
        }
      ],
      "source": [
        "# Create helper.cuh - Helper function declarations\n",
        "helper_cuh = '''\n",
        "#ifndef HELPER_CUH\n",
        "#define HELPER_CUH\n",
        "\n",
        "#include \"kernels.cuh\"\n",
        "#include <chrono>\n",
        "\n",
        "// Host function declarations\n",
        "void convolutionForwardT4(float* input, float* weights, float* output,\n",
        "                         int batch_size, int num_filters, int input_channels,\n",
        "                         int input_height, int input_width, int kernel_size);\n",
        "\n",
        "void convolutionBackward(int batch_size, int num_filters, int input_channels,\n",
        "                        int input_height, int input_width, int kernel_size,\n",
        "                        float* dLdY, float* input, float* weights,\n",
        "                        float* dLdX, float* dLdW);\n",
        "\n",
        "void flashAttentionForwardHost(const float* Q, const float* K, const float* V,\n",
        "                              float* output, int batch_size, int seq_len,\n",
        "                              int head_dim, int num_heads);\n",
        "\n",
        "void flashAttentionBackwardHost(const float* Q, const float* K, const float* V,\n",
        "                               const float* dO, float* dQ, float* dK, float* dV,\n",
        "                               int batch_size, int seq_len, int head_dim, int num_heads);\n",
        "\n",
        "// Utility functions\n",
        "void printGPUInfo();\n",
        "void benchmarkConvolution();\n",
        "void benchmarkFlashAttention();\n",
        "float measureKernelTime(void (*kernel_func)());\n",
        "\n",
        "// Memory management helpers\n",
        "void allocateDeviceMemory(float** ptr, size_t size);\n",
        "void freeDeviceMemory(float* ptr);\n",
        "void copyToDevice(float* dst, const float* src, size_t size);\n",
        "void copyToHost(float* dst, const float* src, size_t size);\n",
        "\n",
        "#endif // HELPER_CUH\n",
        "'''\n",
        "\n",
        "with open('helper.cuh', 'w') as f:\n",
        "    f.write(helper_cuh)\n",
        "\n",
        "print(\"Created helper.cuh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJEg78YuWoI_",
        "outputId": "15786be0-ee84-4543-8e37-d2810554dbbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created kernels.cu\n"
          ]
        }
      ],
      "source": [
        "# Create kernels.cu - CUDA kernels implementation\n",
        "kernels_cu = '''\n",
        "#include \"kernels.cuh\"\n",
        "\n",
        "// T4-optimized unroll kernel\n",
        "__global__ void unrollKernel(const float* input, float* input_unrolled,\n",
        "                            const int input_channels, const int input_height, const int input_width,\n",
        "                            const int kernel_size, const int output_height, const int output_width) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_elements = output_height * output_width;\n",
        "\n",
        "    if (idx < total_elements) {\n",
        "        int out_y = idx / output_width;\n",
        "        int out_x = idx % output_width;\n",
        "\n",
        "        for (int c = 0; c < input_channels; c++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int in_y = out_y + ky;\n",
        "                    int in_x = out_x + kx;\n",
        "\n",
        "                    int unroll_idx = idx * (input_channels * kernel_size * kernel_size) +\n",
        "                                   (c * kernel_size * kernel_size + ky * kernel_size + kx);\n",
        "\n",
        "                    int input_idx = c * (input_height * input_width) +\n",
        "                                  in_y * input_width + in_x;\n",
        "\n",
        "                    input_unrolled[unroll_idx] = input[input_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Standard convolution kernel\n",
        "__global__ void convolutionKernel(const float* input_unrolled, const float* weights, float* output,\n",
        "                                 const int output_size, const int num_filters, const int filter_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < output_size * num_filters) {\n",
        "        int output_idx = idx / num_filters;\n",
        "        int filter_idx = idx % num_filters;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        #pragma unroll 4\n",
        "        for (int i = 0; i < filter_size; i++) {\n",
        "            sum += input_unrolled[output_idx * filter_size + i] *\n",
        "                   weights[i * num_filters + filter_idx];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// T4-optimized convolution kernel with shared memory\n",
        "__global__ void convolutionKernelOptimized(const float* input_unrolled, const float* weights, float* output,\n",
        "                                          const int output_size, const int num_filters, const int filter_size) {\n",
        "    extern __shared__ float shared_weights[];\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // Load weights into shared memory\n",
        "    for (int i = tid; i < filter_size * num_filters; i += blockDim.x) {\n",
        "        if (i < filter_size * num_filters) {\n",
        "            shared_weights[i] = weights[i];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (idx < output_size * num_filters) {\n",
        "        int output_idx = idx / num_filters;\n",
        "        int filter_idx = idx % num_filters;\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        #pragma unroll 4\n",
        "        for (int i = 0; i < filter_size; i++) {\n",
        "            sum += input_unrolled[output_idx * filter_size + i] *\n",
        "                   shared_weights[i * num_filters + filter_idx];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Backward pass kernels\n",
        "template <typename T>\n",
        "__global__ void compute_dLdW(T* dLdY, T* input_unrolled, T* dLdW,\n",
        "                            int output_height, int output_width, int num_filters, int filter_size) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < filter_size && col < num_filters) {\n",
        "        T sum = 0;\n",
        "        for (int i = 0; i < output_height * output_width; i++) {\n",
        "            sum += input_unrolled[i * filter_size + row] * dLdY[i * num_filters + col];\n",
        "        }\n",
        "        dLdW[row * num_filters + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__global__ void compute_dLdX(T* dLdY, T* weights, T* dLdX_unrolled,\n",
        "                            int output_height, int output_width, int num_filters, int filter_size) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height * output_width && col < filter_size) {\n",
        "        T sum = 0;\n",
        "        for (int i = 0; i < num_filters; i++) {\n",
        "            sum += dLdY[row * num_filters + i] * weights[col * num_filters + i];\n",
        "        }\n",
        "        dLdX_unrolled[row * filter_size + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Flash Attention Forward Pass - T4 Optimized\n",
        "__global__ void flashAttentionForward(const float* Q, const float* K, const float* V,\n",
        "                                     float* output, float* l, float* m,\n",
        "                                     int batch_size, int seq_len, int head_dim, int num_heads) {\n",
        "    extern __shared__ float shared_mem[];\n",
        "\n",
        "    int batch_idx = blockIdx.z;\n",
        "    int head_idx = blockIdx.y;\n",
        "    int seq_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (seq_idx >= seq_len) return;\n",
        "\n",
        "    float* shared_K = shared_mem;\n",
        "    float* shared_V = shared_mem + head_dim * BLOCK_SIZE;\n",
        "\n",
        "    int q_offset = batch_idx * num_heads * seq_len * head_dim +\n",
        "                   head_idx * seq_len * head_dim + seq_idx * head_dim;\n",
        "\n",
        "    // Load Q into registers\n",
        "    float q_vec[64]; // Assuming max head_dim = 64\n",
        "    for (int d = 0; d < head_dim; d++) {\n",
        "        q_vec[d] = Q[q_offset + d];\n",
        "    }\n",
        "\n",
        "    float max_score = -INFINITY;\n",
        "    float sum_exp = 0.0f;\n",
        "    float output_vec[64] = {0.0f};\n",
        "\n",
        "    // Process in blocks for memory efficiency\n",
        "    for (int block_start = 0; block_start < seq_len; block_start += BLOCK_SIZE) {\n",
        "        int block_end = min(block_start + BLOCK_SIZE, seq_len);\n",
        "\n",
        "        // Load K and V into shared memory\n",
        "        for (int i = threadIdx.x; i < (block_end - block_start) * head_dim; i += blockDim.x) {\n",
        "            int local_seq = i / head_dim;\n",
        "            int dim = i % head_dim;\n",
        "            int global_seq = block_start + local_seq;\n",
        "\n",
        "            if (global_seq < seq_len) {\n",
        "                int kv_offset = batch_idx * num_heads * seq_len * head_dim +\n",
        "                               head_idx * seq_len * head_dim + global_seq * head_dim + dim;\n",
        "                shared_K[local_seq * head_dim + dim] = K[kv_offset];\n",
        "                shared_V[local_seq * head_dim + dim] = V[kv_offset];\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        // Compute attention scores for this block\n",
        "        for (int k = 0; k < block_end - block_start; k++) {\n",
        "            float score = 0.0f;\n",
        "            for (int d = 0; d < head_dim; d++) {\n",
        "                score += q_vec[d] * shared_K[k * head_dim + d];\n",
        "            }\n",
        "            score /= sqrtf((float)head_dim);\n",
        "\n",
        "            // Update max and running sum\n",
        "            float new_max = fmaxf(max_score, score);\n",
        "            float exp_score = expf(score - new_max);\n",
        "            float exp_old_max = expf(max_score - new_max);\n",
        "\n",
        "            sum_exp = sum_exp * exp_old_max + exp_score;\n",
        "\n",
        "            // Update output\n",
        "            for (int d = 0; d < head_dim; d++) {\n",
        "                output_vec[d] = output_vec[d] * exp_old_max +\n",
        "                               exp_score * shared_V[k * head_dim + d];\n",
        "            }\n",
        "\n",
        "            max_score = new_max;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Normalize and write output\n",
        "    int out_offset = batch_idx * num_heads * seq_len * head_dim +\n",
        "                     head_idx * seq_len * head_dim + seq_idx * head_dim;\n",
        "\n",
        "    for (int d = 0; d < head_dim; d++) {\n",
        "        output[out_offset + d] = output_vec[d] / sum_exp;\n",
        "    }\n",
        "\n",
        "    // Store statistics for backward pass\n",
        "    int stat_offset = batch_idx * num_heads * seq_len + head_idx * seq_len + seq_idx;\n",
        "    l[stat_offset] = sum_exp;\n",
        "    m[stat_offset] = max_score;\n",
        "}\n",
        "\n",
        "// Max Pooling Kernels\n",
        "__global__ void maxPoolingKernel(float* input, float* output,\n",
        "                               int input_height, int input_width, int pool_size, int stride) {\n",
        "    int output_height = (input_height - pool_size) / stride + 1;\n",
        "    int output_width = (input_width - pool_size) / stride + 1;\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height && col < output_width) {\n",
        "        float max_value = -INFINITY;\n",
        "        for (int i = 0; i < pool_size; i++) {\n",
        "            for (int j = 0; j < pool_size; j++) {\n",
        "                int input_row = row * stride + i;\n",
        "                int input_col = col * stride + j;\n",
        "                max_value = fmaxf(max_value, input[input_row * input_width + input_col]);\n",
        "            }\n",
        "        }\n",
        "        output[row * output_width + col] = max_value;\n",
        "    }\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__global__ void maxPoolingBackwardKernel(T* dLdY, T* input, T* dLdX,\n",
        "                                        int input_height, int input_width, int pool_size, int stride) {\n",
        "    int output_height = (input_height - pool_size) / stride + 1;\n",
        "    int output_width = (input_width - pool_size) / stride + 1;\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < output_height && col < output_width) {\n",
        "        T max_value = -INFINITY;\n",
        "        int max_i = -1, max_j = -1;\n",
        "\n",
        "        for (int i = 0; i < pool_size; i++) {\n",
        "            for (int j = 0; j < pool_size; j++) {\n",
        "                int input_row = row * stride + i;\n",
        "                int input_col = col * stride + j;\n",
        "\n",
        "                if (input_row < input_height && input_col < input_width) {\n",
        "                    if (input[input_row * input_width + input_col] > max_value) {\n",
        "                        max_value = input[input_row * input_width + input_col];\n",
        "                        max_i = input_row;\n",
        "                        max_j = input_col;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (max_i != -1 && max_j != -1) {\n",
        "            atomicAdd(&dLdX[max_i * input_width + max_j], dLdY[row * output_width + col]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Explicit template instantiations\n",
        "template __global__ void compute_dLdW<float>(float*, float*, float*, int, int, int, int);\n",
        "template __global__ void compute_dLdX<float>(float*, float*, float*, int, int, int, int);\n",
        "template __global__ void maxPoolingBackwardKernel<float>(float*, float*, float*, int, int, int, int);\n",
        "'''\n",
        "\n",
        "with open('kernels.cu', 'w') as f:\n",
        "    f.write(kernels_cu)\n",
        "\n",
        "print(\"Created kernels.cu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oC4z1FjWoJB",
        "outputId": "af2a0466-7174-4097-b311-36bacbbb01db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created helper.cu\n"
          ]
        }
      ],
      "source": [
        "# Create helper.cu - Helper functions implementation\n",
        "helper_cu = '''\n",
        "#include \"helper.cuh\"\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "\n",
        "// T4-optimized convolution forward pass\n",
        "void convolutionForwardT4(float* input, float* weights, float* output,\n",
        "                         int batch_size, int num_filters, int input_channels,\n",
        "                         int input_height, int input_width, int kernel_size) {\n",
        "    int output_height = input_height - kernel_size + 1;\n",
        "    int output_width = input_width - kernel_size + 1;\n",
        "    int output_size = output_height * output_width;\n",
        "    int filter_size = input_channels * kernel_size * kernel_size;\n",
        "\n",
        "    float* input_unrolled;\n",
        "    size_t unrolled_size = output_size * filter_size * sizeof(float);\n",
        "    CUDA_CHECK(cudaMalloc(&input_unrolled, unrolled_size));\n",
        "\n",
        "    // T4-optimized block sizes\n",
        "    int unroll_threads = 256;\n",
        "    int conv_threads = 256;\n",
        "    int unroll_blocks = (output_size + unroll_threads - 1) / unroll_threads;\n",
        "    int conv_blocks = (output_size * num_filters + conv_threads - 1) / conv_threads;\n",
        "\n",
        "    // Use shared memory if weights fit\n",
        "    size_t shared_mem_size = filter_size * num_filters * sizeof(float);\n",
        "    bool use_shared_memory = (shared_mem_size <= 48 * 1024); // Leave some space for other variables\n",
        "\n",
        "    for (int n = 0; n < batch_size; n++) {\n",
        "        float* input_n = input + n * input_channels * input_height * input_width;\n",
        "        float* output_n = output + n * num_filters * output_height * output_width;\n",
        "\n",
        "        // Launch unroll kernel\n",
        "        unrollKernel<<<unroll_blocks, unroll_threads>>>(\n",
        "            input_n, input_unrolled, input_channels,\n",
        "            input_height, input_width, kernel_size,\n",
        "            output_height, output_width\n",
        "        );\n",
        "        CUDA_CHECK(cudaGetLastError());\n",
        "\n",
        "        // Launch appropriate convolution kernel\n",
        "        if (use_shared_memory) {\n",
        "            convolutionKernelOptimized<<<conv_blocks, conv_threads, shared_mem_size>>>(\n",
        "                input_unrolled, weights, output_n,\n",
        "                output_size, num_filters, filter_size\n",
        "            );\n",
        "        } else {\n",
        "            convolutionKernel<<<conv_blocks, conv_threads>>>(\n",
        "                input_unrolled, weights, output_n,\n",
        "                output_size, num_filters, filter_size\n",
        "            );\n",
        "        }\n",
        "        CUDA_CHECK(cudaGetLastError());\n",
        "        CUDA_CHECK(cudaDeviceSynchronize());\n",
        "    }\n",
        "\n",
        "    cudaFree(input_unrolled);\n",
        "}\n",
        "\n",
        "// Flash Attention host function\n",
        "void flashAttentionForwardHost(const float* Q, const float* K, const float* V,\n",
        "                              float* output, int batch_size, int seq_len,\n",
        "                              int head_dim, int num_heads) {\n",
        "    // Allocate statistics for backward pass\n",
        "    float *d_l, *d_m;\n",
        "    size_t stat_size = batch_size * num_heads * seq_len * sizeof(float);\n",
        "    CUDA_CHECK(cudaMalloc(&d_l, stat_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_m, stat_size));\n",
        "\n",
        "    // T4-optimized grid configuration\n",
        "    dim3 blockSize(min(256, seq_len));\n",
        "    dim3 gridSize((seq_len + blockSize.x - 1) / blockSize.x, num_heads, batch_size);\n",
        "\n",
        "    // Shared memory for K and V blocks\n",
        "    size_t shared_mem_size = 2 * BLOCK_SIZE * head_dim * sizeof(float);\n",
        "\n",
        "    flashAttentionForward<<<gridSize, blockSize, shared_mem_size>>>(\n",
        "        Q, K, V, output, d_l, d_m,\n",
        "        batch_size, seq_len, head_dim, num_heads\n",
        "    );\n",
        "\n",
        "    CUDA_CHECK(cudaGetLastError());\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "\n",
        "    cudaFree(d_l);\n",
        "    cudaFree(d_m);\n",
        "}\n",
        "\n",
        "// GPU info printer\n",
        "void printGPUInfo() {\n",
        "    cudaDeviceProp prop;\n",
        "    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
        "\n",
        "    printf(\"=== GPU Information ===\\\\n\");\n",
        "    printf(\"GPU: %s\\\\n\", prop.name);\n",
        "    printf(\"Compute Capability: %d.%d\\\\n\", prop.major, prop.minor);\n",
        "    printf(\"Global Memory: %.2f GB\\\\n\", prop.totalGlobalMem / (1024.0*1024.0*1024.0));\n",
        "    printf(\"Shared Memory per Block: %zu KB\\\\n\", prop.sharedMemPerBlock / 1024);\n",
        "    printf(\"Multiprocessors: %d\\\\n\", prop.multiProcessorCount);\n",
        "    printf(\"Max Threads per Block: %d\\\\n\", prop.maxThreadsPerBlock);\n",
        "    printf(\"Warp Size: %d\\\\n\", prop.warpSize);\n",
        "    printf(\"Memory Clock Rate: %.2f GHz\\\\n\", prop.memoryClockRate / 1000000.0);\n",
        "    printf(\"Memory Bus Width: %d bits\\\\n\", prop.memoryBusWidth);\n",
        "    printf(\"Peak Memory Bandwidth: %.2f GB/s\\\\n\",\n",
        "           2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6);\n",
        "    printf(\"\\\\n\");\n",
        "}\n",
        "\n",
        "// Benchmark convolution\n",
        "void benchmarkConvolution() {\n",
        "    printf(\"=== Convolution Benchmark ===\\\\n\");\n",
        "\n",
        "    const int batch_size = 4;\n",
        "    const int input_channels = 64;\n",
        "    const int input_height = 128;\n",
        "    const int input_width = 128;\n",
        "    const int kernel_size = 3;\n",
        "    const int num_filters = 128;\n",
        "\n",
        "    const int output_height = input_height - kernel_size + 1;\n",
        "    const int output_width = input_width - kernel_size + 1;\n",
        "\n",
        "    // Allocate memory\n",
        "    size_t input_size = batch_size * input_channels * input_height * input_width * sizeof(float);\n",
        "    size_t weights_size = num_filters * input_channels * kernel_size * kernel_size * sizeof(float);\n",
        "    size_t output_size = batch_size * num_filters * output_height * output_width * sizeof(float);\n",
        "\n",
        "    float *d_input, *d_weights, *d_output;\n",
        "    CUDA_CHECK(cudaMalloc(&d_input, input_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_weights, weights_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output, output_size));\n",
        "\n",
        "    // Initialize with dummy data\n",
        "    CUDA_CHECK(cudaMemset(d_input, 1, input_size));\n",
        "    CUDA_CHECK(cudaMemset(d_weights, 1, weights_size));\n",
        "    CUDA_CHECK(cudaMemset(d_output, 0, output_size));\n",
        "\n",
        "    // Warmup\n",
        "    convolutionForwardT4(d_input, d_weights, d_output,\n",
        "                        batch_size, num_filters, input_channels,\n",
        "                        input_height, input_width, kernel_size);\n",
        "\n",
        "    // Benchmark\n",
        "    const int num_iterations = 100;\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 0; i < num_iterations; i++) {\n",
        "        convolutionForwardT4(d_input, d_weights, d_output,\n",
        "                            batch_size, num_filters, input_channels,\n",
        "                            input_height, input_width, kernel_size);\n",
        "    }\n",
        "\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);\n",
        "\n",
        "    float avg_time_ms = duration.count() / 1000.0f / num_iterations;\n",
        "\n",
        "    // Calculate FLOPS\n",
        "    long long ops_per_conv = 2LL * batch_size * num_filters * output_height * output_width *\n",
        "                            input_channels * kernel_size * kernel_size;\n",
        "    float gflops = (ops_per_conv / (avg_time_ms * 1e6)) * 1000;\n",
        "\n",
        "    printf(\"Input: %dx%dx%dx%d\\\\n\", batch_size, input_channels, input_height, input_width);\n",
        "    printf(\"Filters: %dx%dx%dx%d\\\\n\", num_filters, input_channels, kernel_size, kernel_size);\n",
        "    printf(\"Output: %dx%dx%dx%d\\\\n\", batch_size, num_filters, output_height, output_width);\n",
        "    printf(\"Average time: %.3f ms\\\\n\", avg_time_ms);\n",
        "    printf(\"Performance: %.2f GFLOPS\\\\n\", gflops);\n",
        "    printf(\"Memory throughput: %.2f GB/s\\\\n\",\n",
        "           (input_size + weights_size + output_size) / (avg_time_ms * 1e6));\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_weights);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "\n",
        "// Benchmark Flash Attention\n",
        "void benchmarkFlashAttention() {\n",
        "    printf(\"\\\\n=== Flash Attention Benchmark ===\\\\n\");\n",
        "\n",
        "    const int batch_size = 8;\n",
        "    const int seq_len = 2048;\n",
        "    const int head_dim = 64;\n",
        "    const int num_heads = 12;\n",
        "\n",
        "    size_t qkv_size = batch_size * num_heads * seq_len * head_dim * sizeof(float);\n",
        "\n",
        "    float *d_Q, *d_K, *d_V, *d_output;\n",
        "    CUDA_CHECK(cudaMalloc(&d_Q, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_K, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_V, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output, qkv_size));\n",
        "\n",
        "    // Initialize with dummy data\n",
        "    CUDA_CHECK(cudaMemset(d_Q, 1, qkv_size));\n",
        "    CUDA_CHECK(cudaMemset(d_K, 1, qkv_size));\n",
        "    CUDA_CHECK(cudaMemset(d_V, 1, qkv_size));\n",
        "    CUDA_CHECK(cudaMemset(d_output, 0, qkv_size));\n",
        "\n",
        "    // Warmup\n",
        "    flashAttentionForwardHost(d_Q, d_K, d_V, d_output,\n",
        "                             batch_size, seq_len, head_dim, num_heads);\n",
        "\n",
        "    // Benchmark\n",
        "    const int num_iterations = 50;\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 0; i < num_iterations; i++) {\n",
        "        flashAttentionForwardHost(d_Q, d_K, d_V, d_output,\n",
        "                                 batch_size, seq_len, head_dim, num_heads);\n",
        "    }\n",
        "\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);\n",
        "\n",
        "    float avg_time_ms = duration.count() / 1000.0f / num_iterations;\n",
        "\n",
        "    // Calculate FLOPS (approximate)\n",
        "    long long ops_per_attention = 4LL * batch_size * num_heads * seq_len * seq_len * head_dim;\n",
        "    float gflops = (ops_per_attention / (avg_time_ms * 1e6)) * 1000;\n",
        "\n",
        "    printf(\"Batch size: %d\\\\n\", batch_size);\n",
        "    printf(\"Sequence length: %d\\\\n\", seq_len);\n",
        "    printf(\"Head dimension: %d\\\\n\", head_dim);\n",
        "    printf(\"Number of heads: %d\\\\n\", num_heads);\n",
        "    printf(\"Average time: %.3f ms\\\\n\", avg_time_ms);\n",
        "    printf(\"Performance: %.2f GFLOPS\\\\n\", gflops);\n",
        "    printf(\"Memory throughput: %.2f GB/s\\\\n\",\n",
        "           (4 * qkv_size) / (avg_time_ms * 1e6));\n",
        "\n",
        "    cudaFree(d_Q);\n",
        "    cudaFree(d_K);\n",
        "    cudaFree(d_V);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "'''\n",
        "\n",
        "with open('helper.cu', 'w') as f:\n",
        "    f.write(helper_cu)\n",
        "\n",
        "print(\"Created helper.cu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcmirT3rWoJD",
        "outputId": "f74c9ab6-6417-4c10-9941-cfc8efaf0b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created flash.cu\n"
          ]
        }
      ],
      "source": [
        "# Create flash.cu - Main Flash Attention implementation\n",
        "flash_cu = '''\n",
        "#include \"helper.cuh\"\n",
        "#include <iostream>\n",
        "\n",
        "// Simple test for convolution\n",
        "void testConvolution() {\n",
        "    printf(\"=== Convolution Test ===\\\\n\");\n",
        "\n",
        "    const int batch_size = 1;\n",
        "    const int input_channels = 1;\n",
        "    const int input_height = 4;\n",
        "    const int input_width = 4;\n",
        "    const int kernel_size = 3;\n",
        "    const int num_filters = 2;\n",
        "    const int output_height = input_height - kernel_size + 1;\n",
        "    const int output_width = input_width - kernel_size + 1;\n",
        "\n",
        "    float input[] = {\n",
        "        1, 2, 3, 4,\n",
        "        5, 6, 7, 8,\n",
        "        9, 10, 11, 12,\n",
        "        13, 14, 15, 16\n",
        "    };\n",
        "\n",
        "    float weights[] = {\n",
        "        1, 0, -1,  1, 0, -1,  1, 0, -1,  // Filter 1\n",
        "        0, 1, -1,  0, 1, -1,  0, 1, -1   // Filter 2\n",
        "    };\n",
        "\n",
        "    float *d_input, *d_weights, *d_output;\n",
        "    size_t input_size = batch_size * input_channels * input_height * input_width * sizeof(float);\n",
        "    size_t weights_size = num_filters * input_channels * kernel_size * kernel_size * sizeof(float);\n",
        "    size_t output_size = batch_size * num_filters * output_height * output_width * sizeof(float);\n",
        "\n",
        "    CUDA_CHECK(cudaMalloc(&d_input, input_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_weights, weights_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output, output_size));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_input, input, input_size, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_weights, weights, weights_size, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemset(d_output, 0, output_size));\n",
        "\n",
        "    convolutionForwardT4(d_input, d_weights, d_output,\n",
        "                        batch_size, num_filters, input_channels,\n",
        "                        input_height, input_width, kernel_size);\n",
        "\n",
        "    float* output = new float[output_size/sizeof(float)];\n",
        "    CUDA_CHECK(cudaMemcpy(output, d_output, output_size, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    printf(\"Output:\\\\n\");\n",
        "    for (int f = 0; f < num_filters; f++) {\n",
        "        printf(\"Filter %d:\\\\n\", f);\n",
        "        for (int i = 0; i < output_height; i++) {\n",
        "            for (int j = 0; j < output_width; j++) {\n",
        "                printf(\"%8.1f \", output[f * output_height * output_width + i * output_width + j]);\n",
        "            }\n",
        "            printf(\"\\\\n\");\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    delete[] output;\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_weights);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "\n",
        "// Simple test for Flash Attention\n",
        "void testFlashAttention() {\n",
        "    printf(\"\\\\n=== Flash Attention Test ===\\\\n\");\n",
        "\n",
        "    const int batch_size = 2;\n",
        "    const int seq_len = 8;\n",
        "    const int head_dim = 16;\n",
        "    const int num_heads = 4;\n",
        "\n",
        "    size_t qkv_size = batch_size * num_heads * seq_len * head_dim * sizeof(float);\n",
        "\n",
        "    // Allocate host memory\n",
        "    float* h_Q = new float[qkv_size/sizeof(float)];\n",
        "    float* h_K = new float[qkv_size/sizeof(float)];\n",
        "    float* h_V = new float[qkv_size/sizeof(float)];\n",
        "    float* h_output = new float[qkv_size/sizeof(float)];\n",
        "\n",
        "    // Initialize with simple patterns\n",
        "    for (int i = 0; i < qkv_size/sizeof(float); i++) {\n",
        "        h_Q[i] = 0.1f * (i % 10);\n",
        "        h_K[i] = 0.1f * ((i + 5) % 10);\n",
        "        h_V[i] = 0.1f * ((i + 3) % 10);\n",
        "    }\n",
        "\n",
        "    float *d_Q, *d_K, *d_V, *d_output;\n",
        "    CUDA_CHECK(cudaMalloc(&d_Q, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_K, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_V, qkv_size));\n",
        "    CUDA_CHECK(cudaMalloc(&d_output, qkv_size));\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(d_Q, h_Q, qkv_size, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_K, h_K, qkv_size, cudaMemcpyHostToDevice));\n",
        "    CUDA_CHECK(cudaMemcpy(d_V, h_V, qkv_size, cudaMemcpyHostToDevice));\n",
        "\n",
        "    flashAttentionForwardHost(d_Q, d_K, d_V, d_output,\n",
        "                             batch_size, seq_len, head_dim, num_heads);\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(h_output, d_output, qkv_size, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    printf(\"Attention output (first few values):\\\\n\");\n",
        "    for (int i = 0; i < min(32, (int)(qkv_size/sizeof(float))); i++) {\n",
        "        printf(\"%.4f \", h_output[i]);\n",
        "        if ((i + 1) % 8 == 0) printf(\"\\\\n\");\n",
        "    }\n",
        "    printf(\"\\\\n\");\n",
        "\n",
        "    delete[] h_Q;\n",
        "    delete[] h_K;\n",
        "    delete[] h_V;\n",
        "    delete[] h_output;\n",
        "    cudaFree(d_Q);\n",
        "    cudaFree(d_K);\n",
        "    cudaFree(d_V);\n",
        "    cudaFree(d_output);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printGPUInfo();\n",
        "\n",
        "    testConvolution();\n",
        "    testFlashAttention();\n",
        "\n",
        "    benchmarkConvolution();\n",
        "    benchmarkFlashAttention();\n",
        "\n",
        "    printf(\"\\\\n=== Flash Attention Backprop Complete ===\\\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "with open('flash.cu', 'w') as f:\n",
        "    f.write(flash_cu)\n",
        "\n",
        "print(\"Created flash.cu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1GwmGpEuWoJD"
      },
      "outputs": [],
      "source": [
        "# Compile all files together with T4 optimizations\n",
        "!nvcc -arch=sm_75 -O3 -use_fast_math -o flash_attention_backprop flash.cu helper.cu kernels.cu -I."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTXfUmbqWoJE",
        "outputId": "e67e3dfe-06bb-4834-90e5-893ef0368c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU Information ===\n",
            "GPU: Tesla T4\n",
            "Compute Capability: 7.5\n",
            "Global Memory: 14.74 GB\n",
            "Shared Memory per Block: 48 KB\n",
            "Multiprocessors: 40\n",
            "Max Threads per Block: 1024\n",
            "Warp Size: 32\n",
            "Memory Clock Rate: 5.00 GHz\n",
            "Memory Bus Width: 256 bits\n",
            "Peak Memory Bandwidth: 320.06 GB/s\n",
            "\n",
            "=== Convolution Test ===\n",
            "Output:\n",
            "Filter 0:\n",
            "     6.0    -10.0 \n",
            "     7.0    -11.0 \n",
            "\n",
            "Filter 1:\n",
            "    10.0    -14.0 \n",
            "    11.0    -15.0 \n",
            "\n",
            "\n",
            "=== Flash Attention Test ===\n",
            "Attention output (first few values):\n",
            "0.5391 0.3586 0.4586 0.4285 0.5285 0.4046 0.5046 0.3692 \n",
            "0.4692 0.4391 0.5391 0.3586 0.4586 0.4285 0.5285 0.4046 \n",
            "0.4875 0.3980 0.4980 0.4988 0.5988 0.4118 0.5118 0.3039 \n",
            "0.4039 0.3875 0.4875 0.3980 0.4980 0.4988 0.5988 0.4118 \n",
            "\n",
            "=== Convolution Benchmark ===\n",
            "Input: 4x64x128x128\n",
            "Filters: 128x64x3x3\n",
            "Output: 4x128x126x126\n",
            "Average time: 21.894 ms\n",
            "Performance: 427705.66 GFLOPS\n",
            "Memory throughput: 2.26 GB/s\n",
            "\n",
            "=== Flash Attention Benchmark ===\n",
            "CUDA error at helper.cu:82 - invalid argument\n"
          ]
        }
      ],
      "source": [
        "# Run the complete Flash Attention implementation\n",
        "!./flash_attention_backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmOoVB7_WoJE",
        "outputId": "9aa7a337-b1f0-49b1-bb63-fc466ec714e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root 1070096 Aug 23 05:16 flash_attention_backprop\n",
            "-rw-r--r-- 1 root root    4365 Aug 23 05:16 flash.cu\n",
            "-rw-r--r-- 1 root root    9242 Aug 23 05:16 helper.cu\n",
            "-rw-r--r-- 1 root root    1496 Aug 23 05:16 helper.cuh\n",
            "-rw-r--r-- 1 root root   10154 Aug 23 05:16 kernels.cu\n",
            "-rw-r--r-- 1 root root    2712 Aug 23 05:16 kernels.cuh\n"
          ]
        }
      ],
      "source": [
        "# List all created files\n",
        "!ls -la *.cu *.cuh flash_attention_backprop"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}